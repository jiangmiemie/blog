# 机器学习

## K-邻近算法

这个算法既可以解决分类问题，也可以用于回归问题，但工业上用于分类的情况更多。

KNN先记录所有已知数据，再利用一个距离函数，

找出已知数据中距离未知事件最近的K组数据，

最后按照这K组数据里最常见的类别预测该事件。

```python
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# 创建一些示例数据
X = np.array([[1, 2], [2, 3], [2, 5], [3, 2], [3, 3], [4, 5]])  # 特征
y = np.array([0, 0, 1, 0, 1, 1])  # 目标标签

# 创建K-最近邻分类器
k = 3  # 选择K的值
model = KNeighborsClassifier(n_neighbors=k).fit(X, y)

# 预测新数据点
new_data_point = np.array([[3, 4]])  # 要预测的新数据点
predicted_class = model.predict(new_data_point)

print("预测类别:", predicted_class)
```

### 简单实战

```python
from sklearn.neighbors import KNeighborsClassifier
import numpy as np
from sklearn.model_selection import train_test_split

# 导入鸢尾花数据库
from sklearn.datasets import load_iris

# 加载数据
iris = load_iris()
iris_X = iris.data
iris_y = iris.target

# # 获取前2条数据,从0开始到2结束,不包括2。写法1
# print(iris_X[0:2])

# # 获取前2条数据,从0开始到2结束,不包括2。写法2，省略0
# print(iris_X[:2])

# # 获取前2条数据,从0开始到2结束,不包括2。写法3，省略0
# print(iris_X[:2,:])

# # 获取前2条数据,从0开始到2结束,不包括2。写法4，省略0,只取第一列
# print(iris_X[:2,0])

# # 查看花的类别
# print(iris_y)
# # 查看花的数据
# print(iris_X)
# # 合在一起查看
# for i in zip(iris_X, iris_y):
#     print(i)

# 把数据打乱，并分成测试数据和训练数据，测试数据的比例为30%
X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=0.3)
# 查看训练数据
print(y_train)
# 实例化KNN分类器
knn = KNeighborsClassifier()
# 训练
knn.fit(X_train, y_train)
# 查看对数据的预测
print(knn.predict(X_test))
# 查看真实数据
print(y_test)
```

### 效果评估

```python
right = 0
error = 0
for i in zip(knn.predict(X_test),y_test):
    #print(i)
    if i[0] == i[1]:
        right +=1
    else:
        error +=1

print('正确率：{}%'.format(right/(right+error)*100))
```

## K均值算法

这是一种解决聚类问题的非监督式学习算法。这个方法简单地利用了一定数量的集群（假设K个集群）对给定数据进行分类。同一集群内的数据点是同类的，不同集群的数据点不同类。

还记得你是怎样从墨水渍中辨认形状的么？K均值算法的过程类似，你也要通过观察集群形状和分布来判断集群数量
K均值算法如何划分集群：

1. 从每个集群中选取K个数据点作为质心（centroids）。

2. 将每一个数据点与距离自己最近的质心划分在同一集群，即生成K个新集群。

3. 找出新集群的质心，这样就有了新的质心。

4. 重复2和3，直到结果收敛，即不再有新的质心出现。

怎样确定K的值：

如果我们在每个集群中计算集群中所有点到质心的距离平方和，再将不同集群的距离平方和相加，我们就得到了这个集群方案的总平方和。

我们知道，随着集群数量的增加，总平方和会减少。但是如果用总平方和对K作图，你会发现在某个K值之前总平方和急速减少，但在这个K值之后减少的幅度大大降低，这个值就是最佳的集群数。

### 距离计算公式

一维坐标系中,设A(x1),B(x2),则A,B之间的距离为

|AB|=√[(x1−x2)2]

二维坐标系中,设A(x1,y1),B(x2,y2),则A,B之间的距离为

|AB|=√[(x1−x2)2+(y1−y2)2]

三维坐标系中,设A(x1,y1,z1),B(x2,y2,z2),则A,B之间的距离为

|AB|=√[(x1−x2)2+(y1−y2)2+(z1−z2)2]

........

```python
from sklearn.cluster import KMeans
import numpy as np

# 创建一些示例数据
X = np.array([[1, 2], [2, 3], [2, 5], [3, 2], [3, 3], [4, 5]])

# 创建K均值模型
k = 2  # 指定要分为的簇的数量
model = KMeans(n_clusters=k)

# 拟合模型
model.fit(X)

# 获取簇中心点
cluster_centers = model.cluster_centers_

# 预测每个样本所属的簇
labels = model.labels_

print("簇中心点:", cluster_centers)
print("样本所属簇:", labels)

```

### 简单示例

```python
# 导入必要的库
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import load_iris
# 加载数据
iris = load_iris()
iris_X = iris.data
iris_y = iris.target

# 创建K均值模型
kmeans = KMeans(n_clusters=3)
# 拟合模型，注意看这是无监督学习，这里只填写了数据集，没有给标签。
kmeans.fit(iris_X)

# 获取簇中心和簇标签
centers = kmeans.cluster_centers_
labels = kmeans.labels_
print(iris_y)
print(labels)
# 我们发现他把0、1、2分类成了1、0、2，这是因为K均值算法是无监督学习，他不知道我们的标签是什么，所以他自己给我们分了一套标签。
```

### 效果评估

```python
# 使用列表推导式将0、1、2转换成1、0、2
exchange={0:1,1:0,2:2}
exchange_labels = [exchange[i] if i in exchange else i for i in labels]

right = 0
error = 0
for i in zip(exchange_labels,iris_y):
    if i[0] == i[1]:
        right +=1
    else:
        error +=1

print('正确率：{}%'.format(right/(right+error)*100))
```

### 可视化结果

#### 二维

```python
plt.scatter(iris_X[:, 0], iris_X[:, 3], c=labels)
plt.scatter(centers[:, 0], centers[:,3], c="red", marker="x")
plt.show()
```

#### 三维

```python
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(projection='3d')
 
xs = list(iris_X[:, 0])
ys = list(iris_X[:, 1])
zs = list(iris_X[:, 2])
data_points = [(x, y, z) for x, y, z in zip(xs, ys, zs)]

# 使用 exchange_labels 数据画图
color={0:'green',1:'red',2:'blue'}
colors = [color[i] if i in color else i for i in exchange_labels]
for data, color in zip(data_points, colors):
    x, y, z = data
    ax.scatter(x, y, z, c=color, edgecolors='none', s=30)

# 使用 iris_y 数据画图
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(projection='3d')
color={0:'green',1:'red',2:'blue'}
colors = [color[i] if i in color else i for i in iris_y]
for data, color in zip(data_points, colors):
    x, y, z = data
    ax.scatter(x, y, z, c=color, edgecolors='none', s=30)
# 注意看左下角的蓝点，是分类错误的10%。
```

## 线性回归

线性回归是利用连续性变量来估计实际数值（例如房价，呼叫次数和总销售额等）。

我们通过线性回归算法找出自变量和因变量间的最佳线性关系，图形上可以确定一条最佳直线。

这条最佳直线就是回归线。这个回归关系可以用Y=aX+b 表示。

```python
import numpy as np
from sklearn.linear_model import LinearRegression

# 创建一些示例数据
X = np.array([[1], [2], [3], [4], [5]])  # 自变量
y = np.array([2, 4, 5, 4, 5])  # 因变量
# 创建线性回归模型
model = LinearRegression()

# 拟合模型
model.fit(X, y)

# 打印回归系数和截距
print("回归系数 (斜率):", model.coef_)
print("截距:", model.intercept_)


# 预测新数据点
new_data_point = np.array([[6]])  # 要预测的新数据点
predicted_value = model.predict(new_data_point)
print("预测值:", predicted_value)

```

### 简单示例

```python
from sklearn import datasets
from sklearn.linear_model import LinearRegression
# 加载数据

loaded_data = datasets.fetch_california_housing()
data_X = loaded_data.data
data_y = loaded_data.target

model = LinearRegression()
model.fit(data_X, data_y)

# 预测前四所房屋价格
print(model.predict(data_X[:4, :]))
# 真实价格
print(data_y[:4])
```

### 效果评估

```python
print(model.get_params())# 获取模型参数
print(model.score(data_X, data_y)) # R^2 coefficient of determination
# 这意味着数据集中因变量的 60% 的变异性已得到考虑，而其余 40% 的变异性仍未得到解释。
```

### 查看模型属性

```python
# 打印回归系数和截距
print("回归系数 (斜率):", model.coef_)
print("截距:", model.intercept_)
```

## 决策树

决策树是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一。

决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。

决策树学习通常包括 3 个步骤：特征选择、决策树的生成和决策树的修剪。

决策树学习的损失函数通常是正则化的极大似然函数，决策树学习属于监督学习，可以认为是学习一个分类规则。

```python
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# 创建一些示例数据
X = np.array([[1], [2], [3], [4], [5]])  # 特征
y = np.array([0, 0, 1, 1, 1])  # 目标标签

# 创建决策树分类器
model = DecisionTreeClassifier()

# 拟合模型
model.fit(X, y)

# 预测新数据点
new_data_point = np.array([[6]])  # 要预测的新数据点
predicted_class = model.predict(new_data_point)

print("预测类别:", predicted_class)
```

## 逻辑回归

假设你的一个朋友让你回答一道题。

可能的结果只有两种：你答对了或没有答对。

为了研究你最擅长的题目领域，你做了各种领域的题目。

那么这个研究的结果可能是这样的：

如果是一道十年级的三角函数题，你有70%的可能性能解出它。

但如果是一道五年级的历史题，你会的概率可能只有30%。

逻辑回归就是给你这样的概率结果。

```python
import numpy as np
from sklearn.linear_model import LogisticRegression

# 创建一些示例数据
X = np.array([[1], [2], [3], [4], [5]])  # 自变量
y = np.array([0, 0, 1, 1, 1])  # 因变量，0表示负类，1表示正类

# 创建逻辑回归模型
model = LogisticRegression()

# 拟合模型
model.fit(X, y)

# 预测新数据点
new_data_point = np.array([[6]])  # 要预测的新数据点
predicted_class = model.predict(new_data_point)
predicted_probability = model.predict_proba(new_data_point)

print("预测类别:", predicted_class)
print("预测概率 (负类, 正类):", predicted_probability)

```

## 朴素贝叶斯

这个算法是建立在贝叶斯理论上的分类方法。

它的假设条件是自变量之间相互独立。

简言之，朴素贝叶斯假定某一特征的出现与其它特征无关。

比如说，如果一个水果它是红色的，圆状的，直径大概7cm左右，我们可能猜测它为苹果。

即使这些特征之间存在一定关系，在朴素贝叶斯算法中我们都认为红色，圆状和直径在判断一个水果是苹果的可能性上是相互独立的。

朴素贝叶斯的模型易于建造，并且在分析大量数据问题时效率很高。

虽然模型简单，但很多情况下工作得比非常复杂的分类方法还要好。

```python
from sklearn.naive_bayes import GaussianNB
import numpy as np

# 创建一些示例数据
X = np.array([[1], [2], [3], [4], [5]])  # 特征
y = np.array([0, 0, 1, 1, 1])  # 目标标签

# 创建朴素贝叶斯分类器 (高斯朴素贝叶斯)
model = GaussianNB()

# 拟合模型
model.fit(X, y)

# 预测新数据点
new_data_point = np.array([[6]])  # 要预测的新数据点
predicted_class = model.predict(new_data_point)
predicted_proba = model.predict_proba(new_data_point)

print("预测类别:", predicted_class)
print("类别概率:", predicted_proba)

```

## 随机森林

随机森林是对决策树集合的特有名称。

随机森林里我们有多个决策树（所以叫“森林”）。

为了给一个新的观察值分类，根据它的特征，每一个决策树都会给出一个分类。

随机森林算法选出投票最多的分类作为分类结果。

怎样生成决策树：

1. 如果训练集中有N种类别，则有重复地随机选取N个样本。这些样本将组成培养决策树的训练集。

2. 如果有M个特征变量，那么选取数m << M，从而在每个节点上随机选取m个特征变量来分割该节点。m在整个森林养成中保持不变。

3. 每个决策树都最大程度上进行分割，没有剪枝。

```python

from sklearn.ensemble import RandomForestClassifier
import numpy as np

# 创建一些示例数据
X = np.array([[1, 2], [2, 3], [2, 5], [3, 2], [3, 3], [4, 5]])  # 特征
y = np.array([0, 0, 1, 0, 1, 1])  # 目标标签

# 创建随机森林分类器
n_estimators = 100  # 设置随机森林中的树的数量
model = RandomForestClassifier(n_estimators=n_estimators)

# 拟合模型
model.fit(X, y)

# 预测新数据点
new_data_point = np.array([[3, 4]])  # 要预测的新数据点
predicted_class = model.predict(new_data_point)

print("预测类别:", predicted_class)

```

## 降维算法

作为一名数据科学家，我们手上的数据有非常多的特征。

虽然这听起来有利于建立更强大精准的模型，但它们有时候反倒也是建模中的一大难题。

怎样才能从1000或2000个变量里找到最重要的变量呢？

这种情况下降维算法及其他算法，如决策树，随机森林，PCA，因子分析，相关矩阵，和缺省值比例等，就能帮我们解决难题。

在这个示例中，我们首先导入了scikit-learn库中的PCA类以及NumPy库。

然后，我们创建了一些示例数据 X，这些数据将被用于降维。

接下来，我们创建了一个PCA降维模型，并使用 fit_transform 方法拟合了模型并进行降维。

在这里，我们选择了将数据降维到2维（n_components为2）。

最后，我们打印了原始数据的形状、降维后数据的形状以及降维后的数据。

```python
from sklearn.decomposition import PCA
import numpy as np

# 创建一些示例数据
X = np.array([[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6]])  # 特征矩阵

# 创建PCA降维模型
n_components = 2  # 指定要降维到的维度
model = PCA(n_components=n_components)

# 拟合模型并进行降维
X_reduced = model.fit_transform(X)

print("原始数据形状:", X.shape)
print("降维后数据形状:", X_reduced.shape)
print("降维后数据:")
print(X_reduced)
```

## 支持向量机

这是一个分类算法。

在这个算法中我们将每一个数据作为一个点在一个n维空间上作图（n是特征数），每一个特征值就代表对应坐标值的大小。

比如说我们有两个特征：一个人的身高和发长。我们可以将这两个变量在一个二维空间上作图，图上的每个点都有两个坐标值（这些坐标轴也叫做支持向量）。

在这个示例中，我们首先导入了scikit-learn库中的svm模块以及NumPy库。然后，我们创建了一些示例数据 X 和 y，其中 X 是特征，y 是目标标签。

接下来，我们创建了一个SVM分类器，使用线性核函数（kernel='linear'）。

然后，我们使用 fit 方法拟合了模型，并在新数据点上使用 predict 方法进行预测，以获取新数据点的类别。

```python
from sklearn import svm
import numpy as np

# 创建一些示例数据
X = np.array([[1, 2], [2, 3], [2, 5], [3, 2], [3, 3], [4, 5]])  # 特征
y = np.array([0, 0, 1, 0, 1, 1])  # 目标标签

# 创建SVM分类器
model = svm.SVC(kernel="linear")

# 拟合模型
model.fit(X, y)

# 预测新数据点
new_data_point = np.array([[3, 4]])  # 要预测的新数据点
predicted_class = model.predict(new_data_point)

print("预测类别:", predicted_class)
```

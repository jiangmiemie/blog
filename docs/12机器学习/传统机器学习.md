---
sidebar_position: 3
title: 传统机器学习
---

## K-邻近算法

这个算法既可以解决分类问题，也可以用于回归问题，但工业上用于分类的情况更多。

KNN先记录所有已知数据，再利用一个距离函数，

找出已知数据中距离未知事件最近的K组数据，

最后按照这K组数据里最常见的类别预测该事件。

```python
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# 创建一些示例数据
X = np.array([[1, 2], [2, 3], [2, 5], [3, 2], [3, 3], [4, 5]])  # 特征
y = np.array([0, 0, 1, 0, 1, 1])  # 目标标签

# 创建K-最近邻分类器
k = 3  # 选择K的值
model = KNeighborsClassifier(n_neighbors=k).fit(X, y)

# 预测新数据点
new_data_point = np.array([[3, 4]])  # 要预测的新数据点

# .predicts()方法返回一个数组，数组中包含了预测的类别
predicted_class = model.predict(new_data_point)

print("预测类别:", predicted_class)
```

### 简单实战

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

# 导入鸢尾花数据库
from sklearn.datasets import load_iris

# 加载数据集，数据集包含数据的特征、标签、类别等许多信息
iris = load_iris()
# 获取数据特征（即花的高度、宽度等）
iris_X = iris.data
# 获取数据标签（即花的品种，用0、1、2代替）
iris_y = iris.target
print(len(iris_X)) # 查看数据集的大小

# # 获取前2条数据,从0开始到2结束,不包括2。写法1
# print(iris_X[0:2])

# # 获取前2条数据,从0开始到2结束,不包括2。写法2，省略0
# print(iris_X[:2])

# # 获取前2条数据,从0开始到2结束,不包括2。写法3，省略0
# print(iris_X[:2,:])

# # 获取前2条数据,从0开始到2结束,不包括2。写法4，省略0,只取第一列
# print(iris_X[:2,0])

# # 查看花的类别
# print(iris_y)
# # 查看花的数据
# print(iris_X)
# # 合在一起查看
# print(list(zip(iris_X,iris_y)))

# 把数据打乱，并分成测试数据和训练数据，test_size是测试数据的比例，0.3表示为30%
X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=0.3)

'''
train_test_split详解

此方法会将数据和标签均分成两部分并打乱，一部分用于训练，一部分用于测试。

所以返回的数据有4个，我们用1、2、3、4给他们做上记号.

数据X [------70%---(1)-- | -30%(2)-]
标签y [------70%---(3)-- | -30%(4)-]

与上图对应，依次是:

训练的数据X(1),  测试的数据X(2),
     ↑↓              ↑↓
训练的标签y(3),  测试的标签y(4)

用(1)、(3)喂出一个模型

让模型预测(2)，获得预测结果

将预测结果与(4)进行比较来测试模型的准确率
'''

# 查看训练数据，已经被随机打乱了
# print(y_train)
# 实例化KNN分类器
knn = KNeighborsClassifier()
# .fit()方法用于训练模型，即让模型从数据中学习
knn.fit(X_train, y_train)
# .predicts()方法返回一个数组，数组中包含了预测的类别
print(knn.predict(X_test))
# 查看真实数据
print(y_test)
```

### 效果评估

```python
right = 0
error = 0
for i in zip(knn.predict(X_test),y_test):
    #print(i)
    if i[0] == i[1]:
        right +=1
    else:
        error +=1
print(right,error)
print('正确率：{}%'.format(right/(right+error)*100))
```

### 效果评估的改进

```python
print('正确率：{}%'.format(knn.score(X_test,y_test)*100))

# 正确率：100.0%
```

## K均值算法

这是一种解决聚类问题的非监督式学习算法。这个方法简单地利用了一定数量的集群（假设K个集群）对给定数据进行分类。同一集群内的数据点是同类的，不同集群的数据点不同类。

还记得你是怎样从墨水渍中辨认形状的么？K均值算法的过程类似，你也要通过观察集群形状和分布来判断集群数量
K均值算法如何划分集群：

1. 从每个集群中选取K个数据点作为质心（centroids）。

2. 将每一个数据点与距离自己最近的质心划分在同一集群，即生成K个新集群。

3. 找出新集群的质心，这样就有了新的质心。

4. 重复2和3，直到结果收敛，即不再有新的质心出现。

怎样确定K的值：

如果我们在每个集群中计算集群中所有点到质心的距离平方和，再将不同集群的距离平方和相加，我们就得到了这个集群方案的总平方和。

我们知道，随着集群数量的增加，总平方和会减少。但是如果用总平方和对K作图，你会发现在某个K值之前总平方和急速减少，但在这个K值之后减少的幅度大大降低，这个值就是最佳的集群数。

距离计算公式：

一维坐标系中,设A(x1),B(x2),则A,B之间的距离为

|AB|=√[(x1−x2)2]

二维坐标系中,设A(x1,y1),B(x2,y2),则A,B之间的距离为

|AB|=√[(x1−x2)2+(y1−y2)2]

三维坐标系中,设A(x1,y1,z1),B(x2,y2,z2),则A,B之间的距离为

|AB|=√[(x1−x2)2+(y1−y2)2+(z1−z2)2]

........

```python
from sklearn.cluster import KMeans
import numpy as np

# 创建一些示例数据
X = np.array([[1, 2], [2, 3], [2, 5], [3, 2], [3, 3], [4, 5]])

# 创建K均值模型
k = 2  # 指定要分为的簇的数量
model = KMeans(n_clusters=k)

# 拟合模型
# .fit()方法用于训练模型，即让模型从数据中学习
model.fit(X)

# 获取簇中心点
cluster_centers = model.cluster_centers_

# 预测每个样本所属的簇
labels = model.labels_

print("簇中心点:", cluster_centers)
print("样本所属簇:", labels)

```

### 简单示例

```python
# 导入必要的库
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import load_iris
# 加载数据
iris = load_iris()
iris_X = iris.data
iris_y = iris.target

# 创建K均值模型
kmeans = KMeans(n_clusters=3)
# 拟合模型，注意看这是无监督学习，这里只填写了数据集，没有给标签。
kmeans.fit(iris_X)

# 获取簇中心和簇标签
centers = kmeans.cluster_centers_
labels = kmeans.labels_
print(iris_y)
print(labels)

# 我们发现他把0、1、2分类成了1、0、2，这是因为K均值算法是无监督学习，他不知道我们的标签是什么，所以他自己给我们分了一套标签。
```

### 效果评估

```python
# 使用列表推导式将0、1、2转换成1、0、2
exchange={0:1,1:0,2:2}
exchange_labels = [exchange[i] if i in exchange else i for i in labels]

right = 0
error = 0
for i in zip(exchange_labels,iris_y):
    if i[0] == i[1]:
        right +=1
    else:
        error +=1

print('正确率：{}%'.format(right/(right+error)*100))
```

### 二维可视化结果

```python
# 选取第1、2特征值与中心点
plt.scatter(iris_X[:, 0], iris_X[:, 1], c=labels)
plt.scatter(centers[:, 0], centers[:,1], c="red", marker="x")
plt.title("Kmeans")
plt.show()
# 选取第3、4项特征值与中心点
plt.scatter(iris_X[:, 2], iris_X[:,3], c=labels)
plt.scatter(centers[:, 2], centers[:,3], c="red", marker="x")
plt.show()
```

### 寻找最佳K

```python
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

iris = load_iris()
X = iris.data
y = iris.target

k_range = range(1, 31)
k_scores = []
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    # loss = -cross_val_score(knn, X, y, cv=10, scoring='mean_squared_error') # for regression
    # 10折交叉验证,对于分类问题，scoring参数默认为accuracy，对于回归问题，默认为r2，或mean_squared_error
    # 原理是将数据分成10份，每次取其中一份作为测试集，其余9份作为训练集，进行10次训练和测试，最后取平均值
    # 是一种常用的验证分类性能好坏的方法
    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy') # for classification

    # .mean()方法用于计算平均值
    k_scores.append(scores.mean())

plt.plot(k_range, k_scores)
plt.xlabel('Value of K for KNN')
plt.ylabel('Cross-Validated Accuracy')
plt.show()
```

## 降维算法

作为一名数据科学家，我们手上的数据有非常多的特征。

虽然这听起来有利于建立更强大精准的模型，但它们有时候反倒也是建模中的一大难题。

怎样才能从1000或2000个变量里找到最重要的变量呢？

这种情况下降维算法及其他算法，如决策树，随机森林，PCA，因子分析，相关矩阵，和缺省值比例等，就能帮我们解决难题。

```python
from sklearn.decomposition import PCA
import numpy as np

# 创建一些示例数据
X = np.array([[1, 2, 3],
              [2, 3, 4],
              [3, 4, 5],
              [4, 5, 6]])  # 特征矩阵

# 创建PCA降维模型
n_components = 2  # 指定要降维到的维度
model = PCA(n_components=n_components)

# .fit_transform()方法可以拟合数据，同时进行降维
X_reduced = model.fit_transform(X)

print("原始数据形状:", X.shape)
print("降维后数据形状:", X_reduced.shape)
print("降维后数据:")
print(X_reduced)
```

### 简单示例

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
# 加载数据
iris = load_iris()
iris_X = iris.data
iris_y = iris.target

# 创建PCA降维模型
# 分别降维到2维和3维
model2 = PCA(n_components=2)
model3 = PCA(n_components=3)

# 拟合模型并进行降维
X_reduced2 = model2.fit_transform(iris_X)
X_reduced3 = model3.fit_transform(iris_X)

print(X_reduced2)
print(X_reduced3)
```

### 效果评估

```python
print("原始数据形状:", iris_X.shape)
print("降维后数据形状:", X_reduced2.shape)
print("降维后数据形状:", X_reduced3.shape)
print("降维后数据:")
print(X_reduced2)
print(X_reduced3)
```

### 效果评估

```python
print("原始数据形状:", iris_X.shape)
print("降维后数据形状:", X_reduced2.shape)
print("降维后数据形状:", X_reduced3.shape)
print("降维后数据:")
print(X_reduced2)
print(X_reduced3)
```

### 二维可视化结果

```python
import matplotlib.pyplot as plt
import pandas as pd

#方法1，直接绘制，自动分配颜色
plt.scatter(X_reduced2[:, 0], X_reduced2[:, 1], c=iris_y)
# plt.show()方法会把图像显示出来并清空画布
plt.show() 

#方法2，指定RGB颜色，再绘制
color={0:'#1f77b4',1:'#ff7f0e',2:'#2ca02c'}
# 列表推导式，如果i在color中，就返回color[i]，否则返回i
colors = [color[i] if i in color else i for i in iris_y]
# 绘制散点图，x轴为X_reduced2[:, 0]，y轴为X_reduced2[:, 1]，颜色为colors
plt.scatter(X_reduced2[:, 0], X_reduced2[:, 1], c=colors)
# plt.show()方法会把图像显示出来并清空画布
plt.show() 

# 方法3
# 我们希望把3种不同类型的数据通过设置不同的颜色和形状来把他们区分开来
# r^表示红色的三角形，gx表示绿色的叉号，bo表示蓝色的圆点
color={0:'r^',1:'gx',2:'bo'}
# 列表推导式，如果i在color中，就返回color[i]，否则返回i
colors = [color[i] if i in color else i for i in iris_y]
data = pd.DataFrame(zip(X_reduced2[:, 0],
                        X_reduced2[:, 1],
                        colors),
                    columns=['x','y','label'])
# 按照label列进行分组
for i in data.groupby('label'):
    x = i[1]['x']
    y = i[1]['y']
    c = i[0]
    plt.plot(x, y,c)
# plt.show()方法会把图像显示出来并清空画布
plt.show()

# 相比于之前课程中直接绘制原始数据，降维后的数据更加容易区分。
```

### 三维可视化结果

```python
import matplotlib.pyplot as plt

fig = plt.figure()
# 创建一个3d的画布
ax = fig.add_subplot(projection='3d')

# 从降维后的数据中取出x、y、z三个维度
xs = list(X_reduced3[:, 0])
ys = list(X_reduced3[:, 1])
zs = list(X_reduced3[:, 2])
# 把分类的0、1、2替换为绿色、红色、蓝色
color={0:'green',1:'red',2:'blue'}
colors = [color[i] if i in color else i for i in iris_y]

for x, y, z, c in zip(xs, ys, zs, colors):
    # 绘制散点图
    ax.scatter(x, y, z, c=c)
plt.show()
```

## 逻辑回归

假设你的一个朋友让你回答一道题。

可能的结果只有两种：你答对了或没有答对。

为了研究你最擅长的题目领域，你做了各种领域的题目。

那么这个研究的结果可能是这样的：

如果是一道十年级的三角函数题，你有70%的可能性能解出它。

但如果是一道五年级的历史题，你会的概率可能只有30%。

逻辑回归就是给你这样的概率结果。

逻辑回归是一种统计模型，它使用数学中的逻辑函数或 logit 函数作为 x 和 y 之间的方程式。Logit 函数将 y 映射为 x 的 sigmoid 函数。

![](https://d1.awsstatic.com/sigmoid.bfc853980146c5868a496eafea4fb79907675f44.png)
![](https://d1.awsstatic.com/S-curve.36de3c694cafe97ef4e391ed26a5cb0b357f6316.png)

多个解释变量会影响因变量的值。要对此类输入数据集建模，逻辑回归公式假设不同自变量之间存在线性关系。您可以修改 sigmoid 函数并按如下公式计算最终输出变量

y = f(β0 + β1x1 + β2x2+… βnxn)

符号 β 表示回归系数。当您给它一个其中包含因变量和自变量的已知值的足够大的实验数据集时，logit 模型可以反向计算这些系数值。

逻辑回归分类器更接近KNN，要解决多分类问题时，常常需要针对不同类别分别建立多个模型。

```python
# 绘制逻辑回归的不同回归系数的sigmoid函数

import matplotlib.pyplot as plt
import numpy as np

def sigmoid(x,p=1):
    # 直接返回sigmoid函数
    return 1. / (1. + np.exp(-p*x))
 
def plot_sigmoid(p=1):
    # param:起点，终点，间距
    x = np.arange(-8, 8, 0.2)
    y = sigmoid(x,p)
    plt.plot(x, y)
    plt.show()
 
if __name__ == '__main__':
    plot_sigmoid()
    plot_sigmoid(20)
    plot_sigmoid(0.5)
```

```python
import numpy as np
from sklearn.linear_model import LogisticRegression

# 创建一些示例数据
X = np.array([[1], [2], [3], [4], [5]])  # 自变量
y = np.array([0, 0, 1, 1, 2])  # 因变量，0表示负类，1表示正类

# 创建逻辑回归模型
model = LogisticRegression()

# .fit()方法用于拟合模型，即训练模型x
model.fit(X, y)

# 预测新数据点
new_data_point = np.array([[6]])  # 要预测的新数据点
# .predict()方法预测新数据点的类别
predicted_class = model.predict(new_data_point)
# .predict_proba()方法预测新数据点的概率
predicted_probability = model.predict_proba(new_data_point)

print("预测类别:", predicted_class)
print("预测概率 (负类, 正类):", predicted_probability)
print(type(predicted_probability))
predicted_probability
```

### 简单示例

```python
from sklearn import datasets
from sklearn.model_selection import train_test_split  
from sklearn.preprocessing import StandardScaler  
from sklearn.linear_model import LogisticRegression

# 加载数据集
digits = datasets.load_digits()
# 获取特征和目标变量  
X = digits.data  
y = digits.target 
  
# 数据预处理：随机分割训练集和测试集 , 如果不指定 random_state，每次运行结果都不一样。42为约定俗成的随机数种子
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 数据标准化  
scaler = StandardScaler()  
# .fit_transform()方法先拟合数据，再标准化。和降维算法的语法一致
X_train = scaler.fit_transform(X_train)  
# .transform()方法直接使用在测试集上进行标准化操作
X_test = scaler.transform(X_test)  
  
# 创建Logistic Regression模型  , 如果不指定 random_state，每次运行结果都不一样。42为约定俗成的随机数种子
model = LogisticRegression(random_state=42)  
  
# .fit()方法用于拟合模型，即训练模型
model.fit(X_train, y_train)  
  
# .predict()方法预测新数据点的类别
y_pred = model.predict(X_test)

```

### 效果评估

```python
from sklearn.metrics import accuracy_score

# accuracy_score()方法计算准确率
accuracy = accuracy_score(y_test, y_pred)  
print(f'Accuracy: {accuracy}')
```

### 可视化数据-数据转图片

```python
from matplotlib import pyplot as plt
# 选出预测错误的样本
index = []
# 遍历所有样本
for i in range(len(y_pred)):
    # 判断是否相等
    if y_pred[i] != y_test[i]:
        # 如果不相等,添加到index中:预测值,真实值,图片(注意要变换形状为8*8)
        index.append((
                    y_pred[i], 
                    y_test[i],
                    X_test[i].reshape((8, 8))))

# 创建一个正方形画布
# nrows:子图的行数
# ncols:子图的列数
# print(len(index)) // 10
# 因为一共有10张图片,所以行数为2,列数为5，即2*5=10
fig, ax = plt.subplots(
    nrows=2,
    ncols=5,
)
# 实例化子画布
ax = ax.flatten()
for i in range(len(index)):
    p = index[i][0] # 取出预测值
    a = index[i][1] # 取出真实值
    img = index[i][2] # 取出图片
    # 在子画布上画出图片，格式为灰度图
    ax[i].imshow(img, cmap='Greys')
    ax[i].set_title(f'{p}-{a}')
plt.show()


```

## 线性回归

线性回归是利用连续性变量来估计实际数值（例如房价，呼叫次数和总销售额等）。

我们通过线性回归算法找出自变量和因变量间的最佳线性关系，图形上可以确定一条最佳直线。

这条最佳直线就是回归线。这个回归关系可以用Y=aX+b 表示。

多个数据可以用Y= β0*X0 + β1X1 + β2X2+…… βnXn+ ε 表示。

```python
import numpy as np
from sklearn.linear_model import LinearRegression

# 创建一些示例数据
X = np.array([[1], [2], [3], [4], [5]])  # 自变量
y = np.array([2, 3, 4, 4, 6])  # 因变量
# 创建线性回归模型
model = LinearRegression()

# 拟合模型
model.fit(X, y)

# 打印回归系数和截距
print("回归系数 (斜率):", model.coef_)
print("截距:", model.intercept_)


# 预测新数据点
new_data_point = np.array([[6]])  # 要预测的新数据点
predicted_value = model.predict(new_data_point)
print("预测值:", predicted_value)

```

### 简单示例

```python
from sklearn import datasets
from sklearn.linear_model import LinearRegression

# .fetch_california_housing() 加载加利福尼亚州住房数据集
loaded_data = datasets.fetch_california_housing()
# .data 数据集中的特征数据
data_X = loaded_data.data
# .target 数据集中的标签数据
data_y = loaded_data.target
# 创建线性回归模型
model = LinearRegression()
# 拟合模型
# .fit() 方法接受两个参数：特征数据和标签数据
model.fit(data_X, data_y)

# 预测前四所房屋价格
# .predict() 方法接受一个参数：特征数据
print(model.predict(data_X[:4, :]))
# 真实价格
print(data_y[:4])
```

### 效果评估

```python
print(model.get_params())# 获取模型参数
# //{'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'positive': False}
print(model.score(data_X, data_y))
# // 0.606232685199805
# 这意味着数据集中因变量的 60% 的变异性已得到考虑，而其余 40% 的变异性仍未得到解释。
```

### 查看模型属性

```python
# 打印回归系数和截距
print("回归系数 (斜率):", model.coef_)
print("截距:", model.intercept_)
```

## 决策树

决策树是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一。

决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-else 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。

决策树学习通常包括 3 个步骤：特征选择、决策树的生成和决策树的修剪。

决策树学习的损失函数通常是正则化的极大似然函数，决策树学习属于监督学习，可以认为是学习一个分类规则。

决策树有2种类型：分类树和回归树。分类树的输出是样本的类别，回归树的输出是一个实数。日常人们所说的决策树，通常是指CART决策树，甚至是指CART决策树中的分类树.

```python
from sklearn.datasets import load_iris
from sklearn import tree
import numpy  as np
iris = load_iris() 

clf = tree.DecisionTreeClassifier(min_samples_leaf=15)  
clf = clf.fit(iris.data, iris.target)
# 决策树模型为:先左后右，先上后下 负数表示没有
print("树结构-左节点："+str(clf.tree_.children_left))
print("树结构-右节点："+str(clf.tree_.children_right))
print("节点分裂特征："+str(clf.tree_.feature))
print("节点分裂阈值："+str(np.round(clf.tree_.threshold,2)))
print("节点类别："+str(clf.classes_.take( [ np.argmax(i) for i in clf.tree_.value])))
```

![](https://bbbdata.com/img/cnt/text/ml/202212/a87c5d0c-d3b2-4f33-bd7b-52e6c3cf4271.png)

![](https://bbbdata.com/img/cnt/text/ml/202205/34b41478-e3ca-4bbb-84b4-e02dba1e56c8.png)

### 简单示例

```python
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer
from sklearn.tree import DecisionTreeClassifier

X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

clf = DecisionTreeClassifier(random_state=0)
# 计算最小成本复杂性修剪期间的修剪路径
path = clf.cost_complexity_pruning_path(X_train, y_train)
# 从路径中提取alpha值和相应的决策树
# 剪枝期间子树的有效 alpha
# 代价复杂度剪枝法，实质就是在树的复杂度与准确性之间取得一个平衡点。
# 原理参考：https://blog.csdn.net/ywj_1991/article/details/126846155
ccp_alphas, impurities = path.ccp_alphas, path.impurities

# 在 DecisionTreeClassifier中， 这种剪枝技术是通过成本复杂度参数ccp_alpha来参数化的。更大的ccp_alpha值增加被剪枝的节点数。
clfs = []
# ccp_alphas的值是通过cost_complexity_pruning_path获得的
for ccp_alpha in ccp_alphas:
    # 每个循环创建一个决策树并将其添加到列表中
    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)
    # 拟合决策树
    clf.fit(X_train, y_train)
    # 决策树模型为:先左后右，先上后下 负数表示没有
    print("{} | 树结构-左节点长度：{}，树结构-右节点长度：{}".format(ccp_alpha,len(clf.tree_.children_right),len(clf.tree_.children_left)))

    # 将决策树添加到列表中
    clfs.append(clf)
print("Number of nodes in the last tree is: {} with ccp_alpha: {}".format(
      clfs[-1].tree_.node_count, ccp_alphas[-1]))
# 删除 ccp_alphas的最后一个值， 因为它对应于完全未剪枝的树
clfs = clfs[:-1]
# ccp_alphas也需要删除最后一个值， 因为它是完全未剪枝的树对应的值
ccp_alphas = ccp_alphas[:-1]
```

### 效果评估

```python
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer
from sklearn.tree import DecisionTreeClassifier

X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

clf = DecisionTreeClassifier(random_state=0)
# 计算最小成本复杂性修剪期间的修剪路径
path = clf.cost_complexity_pruning_path(X_train, y_train)
# 从路径中提取alpha值和相应的决策树
# 剪枝期间子树的有效 alpha
# 代价复杂度剪枝法，实质就是在树的复杂度与准确性之间取得一个平衡点。
# 原理参考：https://blog.csdn.net/ywj_1991/article/details/126846155
ccp_alphas, impurities = path.ccp_alphas, path.impurities

# 在 DecisionTreeClassifier中， 这种剪枝技术是通过成本复杂度参数ccp_alpha来参数化的。更大的ccp_alpha值增加被剪枝的节点数。
clfs = []
# ccp_alphas的值是通过cost_complexity_pruning_path获得的
for ccp_alpha in ccp_alphas:
    # 每个循环创建一个决策树并将其添加到列表中
    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)
    # 拟合决策树
    clf.fit(X_train, y_train)
    # 决策树模型为:先左后右，先上后下 负数表示没有
    print("{} | 树结构-左节点长度：{}，树结构-右节点长度：{}".format(ccp_alpha,len(clf.tree_.children_right),len(clf.tree_.children_left)))

    # 将决策树添加到列表中
    clfs.append(clf)
print("Number of nodes in the last tree is: {} with ccp_alpha: {}".format(
      clfs[-1].tree_.node_count, ccp_alphas[-1]))
# 删除 ccp_alphas的最后一个值， 因为它对应于完全未剪枝的树
clfs = clfs[:-1]
# ccp_alphas也需要删除最后一个值， 因为它是完全未剪枝的树对应的值
ccp_alphas = ccp_alphas[:-1]
# 绘制每个alpha值的树的测试集精度和训练集精度
node_counts = [clf.tree_.node_count for clf in clfs]
# 计算每个树的测试集精度和训练集精度
depth = [clf.tree_.max_depth for clf in clfs]


# 绘制精度与alpha的关系
train_scores = [clf.score(X_train, y_train) for clf in clfs]
# 绘制测试集精度
test_scores = [clf.score(X_test, y_test) for clf in clfs]

fig, ax = plt.subplots()
ax.plot(ccp_alphas, train_scores, marker='o', label="train",
        drawstyle="steps-post")
ax.plot(ccp_alphas, test_scores, marker='o', label="test",
        drawstyle="steps-post")
ax.legend()
plt.show()
'''
当 ccp_alpha 设置为0, 并保留DecisionTreeClassifier的其他默认参数时, 树就过拟合了，
使训练的准确率达到100%，测试的准确率达到88%。
随着alpha的增加，更多的树被剪枝，从而创建了一个泛化更好的决策树。
在本例中，设置 ccp_alpha=0.015可以最大限度地提高测试的准确率。
'''
```

### 模型保存

```python
import pickle
import os
# 保存模型
with open('clf_model_v1.pickle','wb') as f:
 pickle.dump(clf,f)
# 加载模型
with open('clf_model_v1.pickle','rb') as f:
 clf2 = pickle.load(f)

# 删除模型
os.remove('clf_model_v1.pickle')

import joblib

# 保存模型
joblib.dump(clf, 'new_app_model_v1.pkl')

# 加载模型
clf3 = joblib.load('new_app_model_v1.pkl')

# 删除模型
os.remove('new_app_model_v1.pkl')
```

## 朴素贝叶斯

这个算法是建立在贝叶斯理论上的分类方法。

它的假设条件是自变量之间相互独立。

简言之，朴素贝叶斯假定某一特征的出现与其它特征无关。即给定类别，特征之间没有相关性。这个假设是“朴素”的来源。

比如说，如果一个水果它是红色的，圆状的，直径大概7cm左右，我们可能猜测它为苹果。即使这些特征之间存在一定关系，在朴素贝叶斯算法中我们都认为红色，圆状和直径在判断一个水果是苹果的可能性上是相互独立的。

一个二分类的案例假设:

我今天收到了100封邮件，其中有80封是垃圾邮件，20封是正常邮件。
P（垃圾邮件） = 80/100 = 0.8
P（正常邮件） = 20/100 = 0.2

我选定了一些词作为特征，这些词可能出现在邮件中，也可能不出现。这些词有“免费”，“恭喜”，“辛苦”等。

我发现垃圾邮件中有20封含有“免费”这个词，50封含有“恭喜”这个词，0封含有“辛苦”这个词。
P（免费|垃圾邮件） = 20/80 = 0.25
P（恭喜|垃圾邮件） = 50/80 = 0.625
P（辛苦|垃圾邮件） = 0/80 = 0

正常邮件中有5封含有“免费”这个词。6封含有“恭喜”这个词，2封含有“辛苦”这个词。
P（免费|正常邮件） = 5/20 = 0.25
P（恭喜|正常邮件） = 6/20 = 0.3
P（辛苦|正常邮件） = 2/20 = 0.1

现在我收到了一封邮件，这封邮件内容为：“恭喜您获得了一次免费的机会”，我想知道这封邮件是垃圾邮件的概率是多少？

P（垃圾邮件|免费，恭喜） = P（免费|垃圾邮件）*P（恭喜|垃圾邮件）* P（垃圾邮件）= 0.25 *0.625* 0.8 = 0.125

P（正常邮件|免费，恭喜） = P（免费|正常邮件）*P（恭喜|正常邮件）* P（正常邮件）= 0.25 *0.3* 0.2 = 0.015

因为P（垃圾邮件|免费，恭喜） > P（正常邮件|免费，恭喜），所以这封邮件被判定为垃圾邮件。

如果狡猾的垃圾邮件制造者把邮件内容改为：“恭喜您获得了一次免费的机会，辛苦您动动手指参加我们的免费活动”，那么这封邮件被判定为垃圾邮件的概率就会变成0，因为“辛苦”这个词在正常邮件中有出现，在垃圾邮件中没有出现。

改进:拉普拉斯平滑法

在每个关键词上人为的增加一个出现的次数，这样就不会出现概率为0的情况了。（下面的公式免费的平方表示这个关键词出现2次）

P（垃圾邮件|免费，恭喜） = P（免费|垃圾邮件）*P（恭喜|垃圾邮件）* P（垃圾邮件）= (21/80)² *(51/80)* 0.8 = 0.0351421875

P（正常邮件|免费，恭喜） = P（免费|正常邮件）*P（恭喜|正常邮件）* P（正常邮件）= (6/20)²*(7/20)* 0.2 =0.0063

```python
from sklearn.naive_bayes import GaussianNB
import numpy as np

# 创建一些示例数据
X = np.array([[1], [2], [3], [4], [5]])  # 特征
y = np.array([0, 0, 1, 1, 1])  # 目标标签

# 创建朴素贝叶斯分类器 (高斯朴素贝叶斯)
model = GaussianNB()

# .fit() 方法用于拟合模型
model.fit(X, y)

# 要预测的新数据点
new_data_point = np.array([[6]]) 
 
# .predict() 方法返回预测的类别
predicted_class = model.predict(new_data_point)
# .predict_proba() 方法返回每个类别的概率
predicted_proba = model.predict_proba(new_data_point)

print("预测类别:", predicted_class)
print("类别概率:", predicted_proba)

```

### 简单示例

```python
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
import sklearn.datasets
# 加载数据
data = sklearn.datasets.load_iris()
# .data 属性包含特征
X = data.data
# .target 属性包含目标标签
y = data.target
# 划分数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
# 创建朴素贝叶斯分类器 (高斯朴素贝叶斯)
model = GaussianNB()
# 拟合模型
model.fit(X_train, y_train)

```

### 效果评估

```python
from sklearn.metrics import accuracy_score  

# 计算准确率  
accuracy = accuracy_score(y_test, model.predict(X_test))  
accuracy
```

### 查看分类错误的样本信息

```python
import pandas as pd

# 把测试数据、目标标签、预测结果合并到一起
# pd.DataFrame()函数用于创建DataFrame
# pd.concat()函数用于合并多个DataFrame
# axis=1 表示按列合并
df = pd.concat(
    [pd.DataFrame(X_test,columns=data.feature_names), 
     pd.DataFrame(y_test,columns=['target']),
     pd.DataFrame(model.predict(X_test),columns=['predict'])
     ],axis=1 )

# 筛选target列与predict列不相等的数据
df.loc[df['target']!=df['predict']]

```

## 支持向量机

这是一个分类算法。

在这个算法中我们将每一个数据作为一个点在一个n维空间上作图（n是特征数），每一个特征值就代表对应坐标值的大小。

比如说我们有两个特征：一个人的身高和发长。我们可以将这两个变量在一个二维空间上作图，图上的每个点都有两个坐标值（这些坐标轴也叫做支持向量）。

在这个示例中，我们首先导入了scikit-learn库中的svm模块以及NumPy库。然后，我们创建了一些示例数据 X 和 y，其中 X 是特征，y 是目标标签。

接下来，我们创建了一个SVM分类器，使用线性核函数（kernel='linear'）。

然后，我们使用 fit 方法拟合了模型，并在新数据点上使用 predict 方法进行预测，以获取新数据点的类别。

```python
from sklearn import svm
import numpy as np

# 创建一些示例数据
X = np.array([[1, 2], [2, 3], [2, 5], [3, 2], [3, 3], [4, 5]])  # 特征
y = np.array([0, 0, 1, 0, 1, 1])  # 目标标签

# 创建SVM分类器
model = svm.SVC(kernel="linear")

# 拟合模型
model.fit(X, y)

# 要预测的新数据点
new_data_point = np.array([[3, 4]])  
# 预测新数据点的类别
predicted_class = model.predict(new_data_point)

print("预测类别:", predicted_class)
```

### 简单示例

```python
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer
from sklearn import svm

# 导入一个数据量较多的数据，乳腺癌
iris = load_breast_cancer()
# 获取数据集
X, y = iris.data ,iris.target

# 直接使用数据
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
# 创建SVM分类器
clf = svm.SVC()
# 拟合模型  
clf.fit(X_train, y_train)
```

### 效果评估

```python
# .score()返回的是准确度
# 和之前学习的accuracy_score、cross_val_score一样都是检测准确度的方法
print('预测是准确度为{}%'.format(clf.score(X_test, y_test)*100))
```

### 数据normalization

```python
from sklearn import preprocessing
# normalization是指将数据按比例缩放，使之落入一个小的特定区间
# 先标准化数据再使用数据
X2 = preprocessing.scale(X)    # normalization step
# print(X2)
X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, test_size=0.3)

clf2 = svm.SVC()
clf2.fit(X2_train, y2_train)
print('预测是准确度为{}%'.format(clf2.score(X2_test, y2_test)*100))
# 简单验证后发现处理后的数据显然表现更好，预测是准确度为98.83040935672514%

from sklearn import model_selection

# 但是并不是每次验证得到的结果都是一致的，验证具有随机性，因此需要交叉验证
# 把数据分成5份，分别做测试集，提取分数并求平均值，显然处理后的数据表现更好
print(model_selection.cross_validate(clf,X_test, y_test,cv=5)['test_score'].mean())
print(model_selection.cross_validate(clf2,X2_test, y2_test,cv=5)['test_score'].mean())
'''
0.8947899159663866 # 未处理的数据
0.9825210084033614 # 处理后的数据
'''
```

### 防止过拟合

```python
from __future__ import print_function
from sklearn.model_selection import  learning_curve
from sklearn.datasets import load_digits
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import numpy as np
# 载入数据
digits = load_digits()
X = digits.data
y = digits.target

# learning_curve()函数用于计算在不同大小的训练集上训练得到的模型在验证集上的得分情况
# 进而分析模型是否过拟合或者欠拟合
# 这里我们使用SVC模型，gamma=0.01
# gamma参数用于控制模型的复杂度，gamma越大，模型越复杂，越容易过拟合
# 通过train_sizes参数来指定训练集的大小
# 通过cv参数来指定交叉验证的次数
# 通过scoring参数来指定评价指标，这里使用的是负均方误差
train_sizes, train_loss, test_loss= learning_curve(
        SVC(gamma=0.01), X, y, cv=10, scoring="neg_mean_squared_error",
        train_sizes=[0.1, 0.25, 0.5, 0.75, 1])

# 计算平均值和标准差
train_loss_mean = -np.mean(train_loss, axis=1)
test_loss_mean = -np.mean(test_loss, axis=1)

# 绘制曲线
# 这里我们使用的是负均方误差，因此数值越小，模型越好
# ro-表示红色圆形实线，go-表示绿色圆形实线
plt.plot(train_sizes, train_loss_mean, 'ro-', 
             label="Training")
plt.plot(train_sizes, test_loss_mean, 'go-', 
             label="test-Cross-validation")
plt.xlabel("Training examples")
plt.ylabel("Loss")
# 显示图例,loc="best"表示自动选择最佳位置
plt.legend(loc="best")
plt.show()
# 这个图表示：
# 刚开始只有200个数据的时候，误差很大，这是因为数据量太少，模型无法很好地拟合数据
# 随着数据量的增加，误差逐渐减小，这是因为模型可以更好地拟合数据
# 但是随着数据集进一步增加，误差反而增大了，这说明模型出现了过拟合
```

## 随机森林

随机森林是对决策树集合的特有名称。

随机森林里我们有多个决策树（所以叫“森林”）。

为了给一个新的观察值分类，根据它的特征，每一个决策树都会给出一个分类。

随机森林算法选出投票最多的分类作为分类结果。

怎样生成决策树：

1. 如果训练集中有N种类别，则有重复地随机选取N个样本。这些样本将组成培养决策树的训练集。

2. 如果有M个特征变量，那么选取数m << M，从而在每个节点上随机选取m个特征变量来分割该节点。m在整个森林养成中保持不变。

3. 每个决策树都最大程度上进行分割，没有剪枝。

```python

from sklearn.ensemble import RandomForestClassifier
import numpy as np

# 创建一些示例数据
X = np.array([[1, 2], [2, 3], [2, 5], [3, 2], [3, 3], [4, 5]])  # 特征
y = np.array([0, 0, 1, 0, 1, 1])  # 目标标签

# 创建随机森林分类器
'''
一般来说，深度越大，拟合效果越好,速度越慢,常用的可以取值10到100之间。
'''
n_estimators = 100  # 设置随机森林中的树的数量
model = RandomForestClassifier(n_estimators=n_estimators)

# 拟合模型
model.fit(X, y)

# 预测新数据点
new_data_point = np.array([[3, 4]])  # 要预测的新数据点
predicted_class = model.predict(new_data_point)

print("预测类别:", predicted_class)

```

### 简单示例

```python
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import make_blobs
from sklearn.ensemble import RandomForestClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import log_loss
# 随机数种子设为0，前面有讲过，这样可以保证每次运行结果都一样
np.random.seed(0)

# 随机生成1000个样本，每个样本包含2个特征，共4个簇
X, y = make_blobs(n_samples=1000, random_state=42, cluster_std=5.0)

# 用前800个样本作为训练集，并且这800个样本中，前600个样本作为训练集，后200个样本用来矫正
X_train, y_train = X[:600], y[:600]
X_valid, y_valid = X[600:800], y[600:800]

# 直接选取前800个样本作为训练集
X_train_valid, y_train_valid = X[:800], y[:800]

# 用前800个样本作为训练+验证集，剩下的200个样本作为测试集
X_test, y_test = X[800:], y[800:]

# 树的数量为25
clf = RandomForestClassifier(n_estimators=25)
# 直接使用前80%的数据训练模型
clf.fit(X_train_valid, y_train_valid)
# 后20%的数据测试，
clf_probs = clf.predict_proba(X_test)
```

### 效果评估

```python
# 用log_loss来评估模型
score = log_loss(y_test, clf_probs)
print("  %.3f " % score)
# 这个分类器对所有800个训练数据点都进行了训练，那么它对它的预测过于自信,导致了过拟合

# 接下来需要矫正这个分类器，让它对自己的预测不那么自信

# 修正前的误差达到了1.3 修正后的数据误差降到了 0.534
```

### 使用模型校准器

```python
# Train random forest classifier, calibrate on validation data and evaluate
# on test data
# 使用前60%的数据训练，中20%的数据校准，后20%的数据测试
clf = RandomForestClassifier(n_estimators=25)
clf.fit(X_train, y_train)
clf_probs = clf.predict_proba(X_test)
# 获取原本的模型
# method="sigmoid" 代表使用sigmoid函数来进行校准
# cv="prefit" 代表使用预先训练好的模型来进行校准
sig_clf = CalibratedClassifierCV(clf, method="sigmoid", cv="prefit")
# 对中间的20%预测后进行概率调试（模型优化）
sig_clf.fit(X_valid, y_valid)
# .predict_proba(X_test) 进行预测(优化后的模型使用方法不变)
sig_clf_probs = sig_clf.predict_proba(X_test)
# 用log_loss来评估模型
sig_score = log_loss(y_test, sig_clf_probs)

print("%.3f" % sig_score)
# 修正前的误差达到了1.3 修正后的数据误差降到了 0.534


```

### 模型校准器工作原理

```python
# Plot changes in predicted probabilities via arrows
plt.figure()
# 颜色表示实例的真正类(红色：1类，绿色：2类，蓝色：3类)
colors = ["r", "g", "b"]
for i in range(clf_probs.shape[0]):
    plt.arrow(clf_probs[i, 0], clf_probs[i, 1],
              sig_clf_probs[i, 0] - clf_probs[i, 0],
              sig_clf_probs[i, 1] - clf_probs[i, 1],
              color=colors[y_test[i]], head_width=1e-2)

# Plot perfect predictions
plt.plot([1.0], [0.0], 'ro', ms=20, label="Class 1")
plt.plot([0.0], [1.0], 'go', ms=20, label="Class 2")
plt.plot([0.0], [0.0], 'bo', ms=20, label="Class 3")

# Plot boundaries of unit simplex
plt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], 'k', label="Simplex")

# Annotate points on the simplex
plt.annotate(r'($\frac{1}{3}$, $\frac{1}{3}$, $\frac{1}{3}$)',
             xy=(1.0/3, 1.0/3), xytext=(1.0/3, .23), xycoords='data',
             arrowprops=dict(facecolor='black', shrink=0.05),
             horizontalalignment='center', verticalalignment='center')
plt.plot([1.0/3], [1.0/3], 'ko', ms=5)
plt.annotate(r'($\frac{1}{2}$, $0$, $\frac{1}{2}$)',
             xy=(.5, .0), xytext=(.5, .1), xycoords='data',
             arrowprops=dict(facecolor='black', shrink=0.05),
             horizontalalignment='center', verticalalignment='center')
plt.annotate(r'($0$, $\frac{1}{2}$, $\frac{1}{2}$)',
             xy=(.0, .5), xytext=(.1, .5), xycoords='data',
             arrowprops=dict(facecolor='black', shrink=0.05),
             horizontalalignment='center', verticalalignment='center')
plt.annotate(r'($\frac{1}{2}$, $\frac{1}{2}$, $0$)',
             xy=(.5, .5), xytext=(.6, .6), xycoords='data',
             arrowprops=dict(facecolor='black', shrink=0.05),
             horizontalalignment='center', verticalalignment='center')
plt.annotate(r'($0$, $0$, $1$)',
             xy=(0, 0), xytext=(.1, .1), xycoords='data',
             arrowprops=dict(facecolor='black', shrink=0.05),
             horizontalalignment='center', verticalalignment='center')
plt.annotate(r'($1$, $0$, $0$)',
             xy=(1, 0), xytext=(1, .1), xycoords='data',
             arrowprops=dict(facecolor='black', shrink=0.05),
             horizontalalignment='center', verticalalignment='center')
plt.annotate(r'($0$, $1$, $0$)',
             xy=(0, 1), xytext=(.1, 1), xycoords='data',
             arrowprops=dict(facecolor='black', shrink=0.05),
             horizontalalignment='center', verticalalignment='center')
# Add grid
plt.grid(False)
for x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:
    plt.plot([0, x], [x, 0], 'k', alpha=0.2)
    plt.plot([0, 0 + (1-x)/2], [x, x + (1-x)/2], 'k', alpha=0.2)
    plt.plot([x, x + (1-x)/2], [0, 0 + (1-x)/2], 'k', alpha=0.2)

plt.title("Change of predicted probabilities after sigmoid calibration")
plt.xlabel("Probability class 1")
plt.ylabel("Probability class 2")
plt.xlim(-0.05, 1.05)
plt.ylim(-0.05, 1.05)
plt.legend(loc="best")

print("Log-loss of")
print(" * uncalibrated classifier trained on 800 datapoints: %.3f "
      % score)
print(" * classifier trained on 600 datapoints and calibrated on "
      "200 datapoint: %.3f" % sig_score)

# Illustrate calibrator
plt.figure()
# generate grid over 2-simplex
p1d = np.linspace(0, 1, 20)
p0, p1 = np.meshgrid(p1d, p1d)
p2 = 1 - p0 - p1
p = np.c_[p0.ravel(), p1.ravel(), p2.ravel()]
p = p[p[:, 2] >= 0]

calibrated_classifier = sig_clf.calibrated_classifiers_[0]
prediction = np.vstack([calibrator.predict(this_p)
                        for calibrator, this_p in
                        zip(calibrated_classifier.calibrators, p.T)]).T
prediction /= prediction.sum(axis=1)[:, None]

# Plot modifications of calibrator
for i in range(prediction.shape[0]):
    plt.arrow(p[i, 0], p[i, 1],
              prediction[i, 0] - p[i, 0], prediction[i, 1] - p[i, 1],
              head_width=1e-2, color=colors[np.argmax(p[i])])
# Plot boundaries of unit simplex
plt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], 'k', label="Simplex")

plt.grid(False)
for x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:
    plt.plot([0, x], [x, 0], 'k', alpha=0.2)
    plt.plot([0, 0 + (1-x)/2], [x, x + (1-x)/2], 'k', alpha=0.2)
    plt.plot([x, x + (1-x)/2], [0, 0 + (1-x)/2], 'k', alpha=0.2)

 # sigmoid校准器图示
plt.title("sigmoid ")
plt.xlabel("Probability class 1")
plt.ylabel("Probability class 2")
# .xlim是设置x轴的范围
plt.xlim(-0.05, 1.05)
# .ylim是设置y轴的范围
plt.ylim(-0.05, 1.05)

plt.show()
```

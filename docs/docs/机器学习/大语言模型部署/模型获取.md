---
sidebar_position: 1
title: 🚧模型获取
---

## 开源社区

大模型社区是指围绕大型深度学习模型（如 GPT 系列、BERT、T5 等）构建的开放协作平台和生态系统。这些社区由研究人员、开发者、数据科学家、工程师及爱好者组成，他们共同致力于大模型的研究、开发、优化和应用。

现在模型非常多，各有千秋，且更新迭代非常快。下面的表格列出了部分公司及其z主要大模型代号：

| **公司名称**                  | **大模型代号**      |
| ----------------------------- | ------------------- |
| **OpenAI**                    | GPT                 |
| **Meta**                      | Llama               |
| **Anthropic(前 OpenAI 成员)** | Claude              |
| **X**                         | Grok                |
| **谷歌**                      | Gemini              |
| **微软**                      | Phi                 |
| **百度**                      | 文心大模型 (Ernie)  |
| **阿里巴巴**                  | 通义千问 (Qwen), M6 |
| **腾讯**                      | 混元 (Hunyuan)      |
| **字节跳动**                  | 豆包                |
| **华为**                      | 盘古大模型 (Pangu)  |

社区具有明显的马太效应，即头部效应明显，头部模型拥有最多的资源，最新的技术，最多的用户。这里列举两个在国内外有一定影响力的社区。

### Hugging Face

社区地址：[https://huggingface.co/](https://huggingface.co/)

以 Qwen 模型为例，下面展示如何使用 Hugging Face 的 transformers 库进行推理。其中`model_name`为模型地址

```python showLineNumbers
from transformers import AutoModelForCausalLM, AutoTokenizer

model_size = "3B"  # 3B 7B 14B 32B
model_name = f"Qwen/Qwen2.5-{model_size}-Instruct"

model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

while True:
    prompt = input("输入你的问题: ")
    if prompt == "退出":
        break

    messages = [
        {
            "role": "system",
            "content": "你是一个AI助手，由阿里巴巴云创建。你是一个乐于助人的助手。你总是以中文回答问题。",
        },
        {"role": "user", "content": prompt},
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_input = tokenizer([text], return_tensors="pt").to(model.device)

    generated_ids = model.generate(**model_input, max_new_tokens=512)
    generated_ids = [output[len(input_ids):] for input_ids, output in zip(model_input.input_ids, generated_ids)]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    print(response)
```

### 魔搭社区（阿里达摩院）

社区地址：[https://www.modelscope.cn/](https://www.modelscope.cn/)

除了 Hugging Face 的 transformers 库，魔搭社区还提供了 modelscope 库，基于中国网络环境，可以方便地进行推理。代码基本与 Hugging Face 一致。

```python showLineNumbers
from modelscope import AutoModelForCausalLM, AutoTokenizer

model_size = "0.5B"  # 3B 7B 14B 32B
model_name = f"Qwen/Qwen2.5-{model_size}-Instruct"

model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

while True:
    prompt = input("输入你的问题: ")
    if prompt == "退出":
        break

    messages = [
        {
            "role": "system",
            "content": "你是一个AI助手，由阿里巴巴云创建。你是一个乐于助人的助手。你总是以中文回答问题。",
        },
        {"role": "user", "content": prompt},
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_input = tokenizer([text], return_tensors="pt").to(model.device)

    generated_ids = model.generate(**model_input, max_new_tokens=512)
    generated_ids = [output[len(input_ids):] for input_ids, output in zip(model_input.input_ids, generated_ids)]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    print(response)
```

## 商用接口

接口大同小异，这里列举一个国内的接口与一个国外的接口用作示例。

### OpenAI

地址：[https://openai.com/](https://openai.com/)

### 百度

地址：[https://cloud.baidu.com/](https://cloud.baidu.com/)

---
sidebar_position: 4
title: 🚧模型微调
---

模型微调（Fine-Tuning） 是指在一个预训练的基础模型上，使用特定领域或特定任务的数据进行进一步训练，以使模型能够在特定任务上表现得更好。例如对计算机科学的名词翻译进行微调，可以提高翻译的准确性。

## 基础知识

微调有三种方法：LoRA 微调、冻结层微调、全量微调。

以微调一个 **14B 参数**的大型语言模型为例，以下是对三种常见微调方法的硬件需求和大致时间的估算：


### LoRA 微调
**硬件需求**：
- **显存**：~24GB显存以上即可（如 NVIDIA RTX 3090 或 A100）。LoRA主要通过冻结大部分模型参数，仅在少量层中插入低秩适配模块，因此显存需求较低。
- **GPU 数量**：单卡或 2 卡即可应付中等规模的微调任务。

**时间估算**：
- **小规模数据**（如数十万条数据，100k steps以内）：几个小时到 1 天。
- **大规模数据**（如百万条数据，300k steps）：2-3 天。
  
**优点**：
- 高效，显存需求低，适合个人开发者或中小型实验室。
- 微调后的模型参数（LoRA 插件）仅几百 MB，方便存储和共享。



### 冻结层微调（Freeze）
**硬件需求**：
- **显存**：需要 48GB 以上显存（如 A6000 或 80GB A100）。冻结大部分模型参数，微调后几层或部分特定模块。
- **GPU 数量**：1-4 张高性能 GPU。

**时间估算**：
- **小规模数据**：1-2 天。
- **大规模数据**：3-5 天。

**优点**：
- 显存需求较低，性能适中。
- 微调时间比全量微调短，但仍需较多资源。



### 全量微调（Full Fine-tuning）
**硬件需求**：
- **显存**：需 80GB 显存以上的高性能 GPU（如 NVIDIA A100 或 H100）。14B 参数模型通常需要 4 卡或更多 GPU 的分布式训练。
- **GPU 数量**：4-8 张 A100（或等效）GPU。对于超大模型，可能需要更多。

**时间估算**：
- **小规模数据**：2-3 天。
- **大规模数据**：5 天到 1 周甚至更长。

**优点**：
- 微调灵活性最高，可以针对特定任务完全优化模型。
- 模型质量可能略高于 LoRA 和冻结层微调。

**缺点**：
- 显存需求高，成本昂贵。
- 微调后模型体积巨大，通常需要数百 GB 存储。



有些框架可以以时间换空间，例如使用梯度检查点：在显存不足时保存中间计算结果，降低瞬时显存需求。或者以精度换空间，例如使用混合精度训练：可以显著减少显存需求和训练时间。有兴趣可以自行搜索。

## LLaMA-Factory

LLaMA-Factory 是基于 LLaMA 的模型微调框架，支持 LoRA 微调、冻结层微调、全量微调。既可以通过 WebUI 微调，也可以通过命令行微调。

项目地址：[https://github.com/hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)

安装

```bash
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install --no-deps -e .
```

启动WebUI

```bash
llamafactory-cli webui
```

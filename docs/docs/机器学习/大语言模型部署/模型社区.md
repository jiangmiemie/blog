---
sidebar_position: 1
title: 模型社区
---

## 开源社区

大模型社区是指围绕大型深度学习模型（如 GPT 系列、BERT、T5 等）构建的开放协作平台和生态系统。这些社区由研究人员、开发者、数据科学家、工程师及爱好者组成，他们共同致力于大模型的研究、开发、优化和应用。

现在模型非常多，各有千秋，且更新迭代非常快。下面的表格列出了部分公司及其主要大模型代号：

| **公司名称**                  | **大模型代号**      |
| ----------------------------- | ------------------- |
| **OpenAI**                    | GPT                 |
| **Meta**                      | Llama               |
| **Anthropic(前 OpenAI 成员)** | Claude              |
| **X**                         | Grok                |
| **谷歌**                      | Gemini              |
| **微软**                      | Phi                 |
| **百度**                      | 文心大模型 (Ernie)  |
| **阿里巴巴**                  | 通义千问 (Qwen), M6 |
| **腾讯**                      | 混元 (Hunyuan)      |
| **字节跳动**                  | 豆包                |
| **华为**                      | 盘古大模型 (Pangu)  |

社区具有明显的马太效应，即头部效应明显，头部模型拥有最多的资源，最新的技术，最多的用户。这里列举两个在国内外有一定影响力的社区。

### Hugging Face

社区地址：[https://huggingface.co/](https://huggingface.co/)

以 Qwen 模型为例，下面展示如何使用 Hugging Face 的 transformers 库进行推理。其中`model_name`为模型地址

```python showLineNumbers
from transformers import AutoModelForCausalLM, AutoTokenizer

model_size = "3B"  # 3B 7B 14B 32B
model_name = f"Qwen/Qwen2.5-{model_size}-Instruct"

model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

while True:
    prompt = input("输入你的问题: ")
    if prompt == "退出":
        break

    messages = [
        {
            "role": "system",
            "content": "你是一个AI助手，由阿里巴巴云创建。你是一个乐于助人的助手。你总是以中文回答问题。",
        },
        {"role": "user", "content": prompt},
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_input = tokenizer([text], return_tensors="pt").to(model.device)

    generated_ids = model.generate(**model_input, max_new_tokens=512)
    generated_ids = [output[len(input_ids):] for input_ids, output in zip(model_input.input_ids, generated_ids)]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    print(response)
```

### 魔搭社区（阿里达摩院）

社区地址：[https://www.modelscope.cn/](https://www.modelscope.cn/)

除了 Hugging Face 的 transformers 库，魔搭社区还提供了 modelscope 库，基于中国网络环境，可以方便地进行推理。代码基本与 Hugging Face 一致。

```python showLineNumbers
from modelscope import AutoModelForCausalLM, AutoTokenizer

model_size = "0.5B"  # 3B 7B 14B 32B
model_name = f"Qwen/Qwen2.5-{model_size}-Instruct"

model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

while True:
    prompt = input("输入你的问题: ")
    if prompt == "退出":
        break

    messages = [
        {
            "role": "system",
            "content": "你是一个AI助手，由阿里巴巴云创建。你是一个乐于助人的助手。你总是以中文回答问题。",
        },
        {"role": "user", "content": prompt},
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_input = tokenizer([text], return_tensors="pt").to(model.device)

    generated_ids = model.generate(**model_input, max_new_tokens=512)
    generated_ids = [output[len(input_ids):] for input_ids, output in zip(model_input.input_ids, generated_ids)]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    print(response)
```

### 模型常见后缀

大部分模型基本有代号、模型类型和参数大小三部分组成，例如：`Qwen2.5-Coder-3B`，其中`Qwen2.5`为代号，`Coder`为模型类型，`3B`为参数大小。

#### 模型类型

- `Instruct`：指令优化  
- `Fine-tuned`：领域微调  
- `Base`：基础模型  
- `Chat`：对话优化  
- `RAG`：检索增强生成  
- `Sparse`：稀疏模型  
- `Dense`：密集模型  
- `Multimodal`：多模态输入  
- `Enhanced` / `Extended`：功能改进或扩展  
- `Lite` / `Compact`：轻量化模型  
- `Instruction-following (IF)`：指令执行优化  
- `Code` / `Coder`：代码优化  
- `Knowledge`：知识问答优化  
- `Custom` / `Domain-Specific`：定制领域模型  
- `Aligned`：人类反馈对齐（通过使用人类反馈调整和优化后的模型）

#### 模型微调后缀

- `GGUF`：高效二进制通用量化格式
- `AWQ`：微调后量化：适应不同场景
- `GPTQ`：分组量化：精度换速度

#### 量化精度

- INT8 (8-bit Integer)：INT8通过将模型权重量化为8位整数，显著减少存储和计算需求，适用于大部分推理任务，但会带来一定的精度损失。
- INT4 (4-bit Integer)：INT4进一步减少存储和计算需求，能提供更快的推理速度和更低的内存占用，但精度损失更大，且支持较为有限。
- FP16 (16-bit Floating Point)：FP16通过使用16位浮点数减少存储需求，精度损失较小，但计算开销高于INT8和INT4，适合精度要求较高的任务。
---
sidebar_position: 1
title: 🚧模型微调与部署
---

## 模型微调

### LLaMA-Factory

## 提示词工程

### Prompt Engineering Guide

[https://www.promptingguide.ai/zh](https://www.promptingguide.ai/zh)

## 模型部署


| 维度                   | Ollama                                             | VLLM                                            | Llama-Cpp-Python                                |
|------------------------|---------------------------------------------------|------------------------------------------------|------------------------------------------------|
| **核心功能**           | 模型管理和推理框架，支持快速加载、切换模型         | 高性能推理引擎，优化 Transformer 模型推理       | 使用 C++ 实现 LLaMA 模型的高效推理，适配多平台  |
| **主要特点**           | 易用的命令行工具，支持多个预训练模型               | 动态批次合并（dynamic batching），高吞吐量推理   | 内存优化，支持量化模型，轻量级和易部署          |
| **性能**               | 适中，重点在于易用性                              | 高，专为分布式和高吞吐量推理优化                | 高，专注于 CPU/GPU 的高效推理                   |
| **支持硬件**           | CPU/GPU                                           | GPU                                            | CPU/GPU                                         |
| **异步支持**           | 支持多任务异步加载模型                             | 原生支持                                        | 支持，但需在应用层实现                          |
| **扩展性**             | 易于集成到 Python 应用中，支持 REST API             | 面向大规模分布式推理设计，扩展性强              | 模块化设计，适合轻量化部署                      |
| **生态和社区**         | 新兴工具，生态尚在发展                            | 成长中，已有一定的研究和工业实践支持            | 开源社区活跃，广泛应用于多种 LLM 项目           |
| **学习曲线**           | 低，简单易用，适合快速上手                        | 中，适合熟悉分布式推理的用户                    | 中，需要了解量化和高效推理相关概念              |
| **部署难度**           | 低，支持简单命令行部署                             | 中，需要配置分布式推理环境                      | 低，单机或轻量化部署简单                        |
| **模型兼容性**         | 支持多个预训练模型，如 GPT 类                     | 支持多种 Transformer 模型，如 GPT、BERT         | 专为 Meta 的 LLaMA 系列模型设计                 |
| **内存优化**           | 支持基础内存管理                                   | 通过动态批次和显存优化提高吞吐量                | 支持量化和裁剪，内存占用极低                    |
| **文档和支持**         | 提供官方文档，支持 CLI 和简单的 REST 接口           | 提供详细文档，面向研究和工业实践                | 文档丰富，社区提供大量样例代码                  |
| **适用场景**           | 小型项目、模型快速切换、开发测试                   | 高性能推理、分布式推理、服务大规模用户          | 内存受限环境、高效推理、小型项目                |
| **对 Python 的支持**   | 支持，通过 CLI 或 REST API 与 Python 集成           | 强支持，直接集成到 Python 项目                  | 专注于 Python 绑定，直接调用 C++ 接口            |
| **成熟度**             | 新兴工具，功能逐步完善                            | 工业级项目，专注高性能推理                      | 成熟项目，广泛使用于轻量化 LLM 应用             |


### ollma

github地址：https://github.com/ollama/ollama

官网：https://ollama.com/

### vllma


官网：https://docs.vllm.ai/en/latest/getting_started/quickstart.html


### llama-cpp-python


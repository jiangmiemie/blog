---
sidebar_position: 3
title: 模型部署
---


模型部署主要有如下几个需求：并发高、延迟低、占用小。解决方案对应Ollama、VLLM、Llama-Cpp-Python。

| 维度                 | Ollama                                     | VLLM                                           | Llama-Cpp-Python                               |
| -------------------- | ------------------------------------------ | ---------------------------------------------- | ---------------------------------------------- |
| **核心功能**         | 模型管理和推理框架，支持快速加载、切换模型 | 高性能推理引擎，优化 Transformer 模型推理      | 使用 C++ 实现 LLaMA 模型的高效推理，适配多平台 |
| **主要特点**         | 易用的命令行工具，支持多个预训练模型       | 动态批次合并（dynamic batching），高吞吐量推理 | 内存优化，支持量化模型，轻量级和易部署         |
| **性能**             | 适中，重点在于易用性                       | 高，专为分布式和高吞吐量推理优化               | 高，专注于 CPU/GPU 的高效推理                  |
| **支持硬件**         | CPU/GPU                                    | GPU                                            | CPU/GPU                                        |
| **异步支持**         | 支持多任务异步加载模型                     | 原生支持                                       | 支持，但需在应用层实现                         |
| **扩展性**           | 易于集成到 Python 应用中，支持 REST API    | 面向大规模分布式推理设计，扩展性强             | 模块化设计，适合轻量化部署                     |
| **生态和社区**       | 新兴工具，生态尚在发展                     | 成长中，已有一定的研究和工业实践支持           | 开源社区活跃，广泛应用于多种 LLM 项目          |
| **学习曲线**         | 低，简单易用，适合快速上手                 | 中，适合熟悉分布式推理的用户                   | 中，需要了解量化和高效推理相关概念             |
| **部署难度**         | 低，支持简单命令行部署                     | 中，需要配置分布式推理环境                     | 低，单机或轻量化部署简单                       |
| **模型兼容性**       | 支持多个预训练模型，如 GPT 类              | 支持多种 Transformer 模型，如 GPT、BERT        | 专为 Meta 的 LLaMA 系列模型设计                |
| **内存优化**         | 支持基础内存管理                           | 通过动态批次和显存优化提高吞吐量               | 支持量化和裁剪，内存占用极低                   |
| **文档和支持**       | 提供官方文档，支持 CLI 和简单的 REST 接口  | 提供详细文档，面向研究和工业实践               | 文档丰富，社区提供大量样例代码                 |
| **适用场景**         | 小型项目、模型快速切换、开发测试           | 高性能推理、分布式推理、服务大规模用户         | 内存受限环境、高效推理、小型项目               |
| **对 Python 的支持** | 支持，通过 CLI 或 REST API 与 Python 集成  | 强支持，直接集成到 Python 项目                 | 专注于 Python 绑定，直接调用 C++ 接口          |
| **成熟度**           | 新兴工具，功能逐步完善，适合单人AI问答     | 工业级项目，专注高性能推理                     | 成熟项目，广泛使用于轻量化 LLM 应用            |


## ollma

github地址：https://github.com/ollama/ollama

官网：https://ollama.com/

常见问题：https://github.com/ollama/ollama/blob/main/docs/faq.md

### ollama部署huggingface模型

```bash showLineNumbers
ollama run hf.co/{username}/{reponame}:latest
```

示例1:运行最新的模型
```bash showLineNumbers
ollama run hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:latest
```

示例2:运行特定的量化模型
```bash showLineNumbers
ollama run hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:Q8_0
```

## vllm

github地址：https://github.com/vllm-project/vllm

官网：https://vllm.ai/

### vllm部署huggingface模型

```bash showLineNumbers
vllm serve NousResearch/Meta-Llama-3-8B-Instruct --dtype auto --api-key token-abc123
```

### vllm部署本地模型

示例1：调用8卡推理
```bash showLineNumbers
vllm serve /home/ly/qwen2.5/Qwen2.5-32B-Instruct/ --tensor-parallel-size 8 --dtype auto --api-key 123 --gpu-memory-utilization 0.95 --max-model-len 27768  --enable-auto-tool-choice --tool-call-parser hermes --served-model-name Qwen2.5-32B-Instruct --kv-cache-dtype fp8_e5m2
```
示例2：指定某块GPU运行模型
```bash showLineNumbers
CUDA_VISIBLE_DEVICES=2 vllm serve /home/ly/qwen2.5/Qwen2-VL-7B-Instruct --dtype auto --tensor-parallel-size 1 auto --api-key 123 --gpu-memory-utilization 0.5 --max-model-len 5108  --enable-auto-tool-choice --tool-call-parser hermes --served-model-name Qwen2-VL-7B-Instruct --port 1236
```

Vllm不支持启动一个服务就可以随机切换其他模型（ollama支持）。

你通常需要为每一个模型单独运行一次vllm命令，并且每个模型都要提供不同的端口，比如他默认的是8000端口，而我上一个命令使用的是1236端口

